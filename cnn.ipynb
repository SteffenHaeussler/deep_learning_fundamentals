{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import NeuralNetwork, Dense, Layer, Linear, Loss, Operation, ParamOperation, Optimizer, SGDMomentum, Tanh, Trainer, Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP nneds more attention - stucked for two weeks and decided to move on/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)\n",
    "X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)\n",
    "\n",
    "X_train_conv, X_test_conv = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "num_labels = len(y_train)\n",
    "train_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][y_train[i]] = 1\n",
    "\n",
    "num_labels = len(y_test)\n",
    "test_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuray(model, X_test, y_test):\n",
    "    return np.equal(np.argmax(model.forward(X_test, inference=True), axis=1), y_test).sum() * 100.0 / X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional operation\n",
    "\n",
    "only simplified for 1D convolution to understand the pattern; \n",
    "for 2D convolution, please look at the comments in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the convolution, let's say we have an image, with the size of 5x5 pixel\n",
    "\n",
    "$$ I = \\begin{bmatrix}i_{11} & i_{12} & i_{13} & i_{14} & i_{15}\\\\\n",
    "                      i_{21} & i_{22} & i_{23} & i_{24} & i_{25}\\\\\n",
    "                      i_{31} & i_{32} & i_{33} & i_{34} & i_{35}\\\\\n",
    "                      i_{41} & i_{42} & i_{43} & i_{44} & i_{45}\\\\\n",
    "                      i_{51} & i_{52} & i_{53} & i_{54} & i_{55}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "and we want to calculate a new feature with a kernel of the size 3x3. Therefor we set the weights W:\n",
    "\n",
    "\n",
    "$$ w = \\begin{bmatrix}w_{11} & w_{12} & w_{13}\\\\w_{21} & w_{22} & w_{23} \\\\w_{31} & w_{32} & w_{33}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "So, we take basically the dot product between multiple 3x3 patches from the Image and the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a 1d array of length 5 $ arr_1 = [i_1, i_2, i_3, i_4, i_5] $ and a kernel with length 3 $ k_1 = [w_1, w_2, w_3] $ we get following output: </br>\n",
    "\n",
    "\n",
    "$$ o_1 = i_1 w_1 + i_2 w_2 + i_3 w_3 $$\n",
    "$$ o_2 = i_2 w_1 + i_3 w_2 + i_4 w_3 $$\n",
    "$$ o_3 = i_3 w_1 + i_4 w_2 + i_5 w_3 $$\n",
    "\n",
    "The output shrinks to three elements from five in the beginning. To prevent this shrinkage, **padding** will be introduced. For this reason,  $ i_0 , i_6 = 0 $ will be introduced. So, we get:\n",
    "\n",
    "$$ o_0 = i_0 w_1 + i_1 w_2 + i_2 w_3 $$\n",
    "$$ o_4 = i_4 w_1 + i_5 w_2 + i_6 w_3 $$\n",
    "\n",
    "with the same output size, as the input. In this case, the **stride** was 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2D convolution, the adaption is to pad the input in an appriopriately way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's take $i_5$ from the output. It is used two times in this example as part of $o_3$ and $o_4$; multiplied with $w_3$ and $w_2$. So it can be written as, with an hypothetical $o_5$ and $w_1$:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial o_4} w_3 + \\frac{\\partial L}{\\partial o_5} w_2 + \\frac{\\partial L}{\\partial o_6} w_1  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z_{i,j,k} = b_k + \\sum_{u=0}^{f_h-1} \\sum_{v=0}^{f_w-1} \\sum_{k'=0}^{f_{n'}-1} x_{i',j',k'} \\cdot w_{u,v,k', k} \\text{with}\n",
    "\\begin{cases}\n",
    "      i' = i \\times s_h + u\\\\\n",
    "      j' = j \\times s_w + v\\\\\n",
    " \\end{cases}\n",
    " $$\n",
    " \n",
    "\n",
    "> - z_{i,j,k} is the output if the neuron located in row i, column j and feature map k of the convolutional layer l\n",
    "> - $s_h \\text{and} s_w$ are vertical and horizontal strides\n",
    "> - $f_h \\text{and} f_w$ are height and width of the receptive field\n",
    "> - $f_{n'}$ is the number of feature maps in the previous layer\n",
    "> - $x_{i',j',k'}$ output of the neuron located in layer l - 1, row i', column j', feature map k'\n",
    "> - $b_k$ bias term for the feature map k\n",
    "> - $w_{u,v,k', k}$ weight between any neuron in feature map k of layer l and its input located at row u, column v and feature map k'\n",
    "\n",
    "Basically, a convolutional layer gets:\n",
    "\n",
    "- as input: [batch_size, in_channels, out_channels, img_height, img_width]\n",
    "- convolves it to params with [in_channels, out_channels, param_height, param_width]\n",
    "- outputs it to [batch_size, in_channels, out_channels, param_height, param_width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers are used to downsample a feature. This reduces the number of weights, memory usage and the computational time. There are two main pooling types:\n",
    "\n",
    "- max_pooling, which takes the maximum out of the pooling kernel\n",
    "- average_pooling, which calculates the average of the pooling kernel\n",
    "\n",
    "Also, for me it is not clear, if pooling layers will be used in the future, since the computational \"power\" is increasing with gpus, tpus and this constraint seems to diminish. e.g. ResNet architecture uses pooling layers very sparsly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flatten operator transforms the output of a convolutional layer (e.g. 3D array) in a vector. \n",
    "This operator reduces the kernels to a vector, where we can feed the vector to a fully connected layer. Basically, the flatten operator allows to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    \n",
    "    def __init__(self, out_channels: int, param_size: int, activation: Operation = Sigmoid(),\n",
    "                 flatten: bool = False) -> None:\n",
    "        \"\"\"requires an activation function upon initialization\"\"\"\n",
    "\n",
    "        super().__init__(out_channels)\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "        \n",
    "        self.param_size = param_size\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        defines options for a fully connected layer\n",
    "        \"\"\"\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        conv_param = np.random.normal(loc=0,\n",
    "                                      size=(input_.shape[1],  # input channels\n",
    "                                            self.out_channels,\n",
    "                                            self.param_size,\n",
    "                                            self.param_size))\n",
    "\n",
    "        self.params.append(conv_param)\n",
    "\n",
    "        self.operations.append(Conv2D_Op(conv_param))\n",
    "        self.operations.append(self.activation)\n",
    "        \n",
    "        if self.flatten:\n",
    "            self.operations.append(Flatten())\n",
    "\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D_Op(ParamOperation):\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        super().__init__(weights)\n",
    "        \n",
    "        self.param_size = weights.shape[2]\n",
    "        # simplification for training purpose\n",
    "        self.param_pad = self.param_size // 2\n",
    "\n",
    "    def _pad_1d(self, input_: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        pads zeros around the input based on the padding size\n",
    "        \"\"\"\n",
    "        zeros = np.repeat(np.array([0]), self.param_pad)\n",
    "        return np.concatenate([zeros, input_, zeros])\n",
    "\n",
    "    def _pad_1d_batch(self, input_: ndarray) -> ndarray:\n",
    "    \n",
    "        # padding for 2D conv; each batch is treated as 1D sequence \n",
    "        output = [self._pad_1d(obs) for obs in input_]\n",
    "\n",
    "        return np.stack(output)\n",
    "        \n",
    "    def _pad_2d_obs(self, input_: ndarray):\n",
    "\n",
    "        input_pad = self._pad_1d_batch(input_)\n",
    "\n",
    "        other = np.zeros((self.param_pad, input_.shape[0] + self.param_pad * 2))\n",
    "\n",
    "        return np.concatenate([other, input_pad, other])\n",
    "        \n",
    "    def _pad_2d_channel(self, input_: ndarray):\n",
    "\n",
    "        return np.stack([self._pad_2d_obs(channel) for channel in input_])\n",
    "    \n",
    "    def _get_image_patches(self, input_: ndarray):\n",
    "\n",
    "        patches = []\n",
    "\n",
    "        # pad the images per batch\n",
    "        imgs_batch_pad = np.stack([self._pad_2d_channel(obs) for obs in input_])\n",
    "\n",
    "        img_height = imgs_batch_pad.shape[2]\n",
    "\n",
    "        #for each location in the image\n",
    "        for h in range(img_height-self.param_size+1):\n",
    "            for w in range(img_height-self.param_size+1):\n",
    "\n",
    "                # get an image patch of the parameter size\n",
    "                patch = imgs_batch_pad[:, :, h:h+self.param_size, w:w+self.param_size]\n",
    "                patches.append(patch)\n",
    "\n",
    "        return np.stack(patches)\n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        FORWARD PASS!\n",
    "        \n",
    "        1. step: Get image patches of size [batch_size, in_channels, img_height * img_width, kernel_size, kernel_size]\n",
    "        2. step: reshape image patches to [batch_size, img_height * img_width, in_channels * kernel_size * kernel_size]\n",
    "        3. step: reshape parameter [in_channels * kernel_size * kernel_size, out_channels]\n",
    "        4. step: batch matrix multiplication [batch_size, img_height * img_width, out_channels]\n",
    "        5. step: reshape to [barch_size, out_channels, img_height, img_width]\n",
    "        \"\"\"\n",
    "        batch_size = self.input_.shape[0]\n",
    "\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        img_height = self.input_.shape[2]\n",
    "\n",
    "        patch_size = self.param.shape[0] * self.param.shape[2] * self.param.shape[3]\n",
    "\n",
    "        #step 1 and 2\n",
    "        output_patches_reshaped = (self._get_image_patches(self.input_)\n",
    "                                  .transpose(1, 0, 2, 3, 4)\n",
    "                                  .reshape(batch_size, img_size, -1))\n",
    "\n",
    "        # step 3\n",
    "        param_reshaped = (self.param\n",
    "                          .transpose(0, 2, 3, 1)\n",
    "                          .reshape(patch_size, -1))\n",
    "\n",
    "        # step 4 and step 5\n",
    "        return (np.matmul(output_patches_reshaped, param_reshaped)\n",
    "                .reshape(batch_size, img_height, img_height, -1)\n",
    "                .transpose(0, 3, 1, 2))\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        batch_size = self.input_.shape[0]\n",
    "\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        img_height = self.input_.shape[2]\n",
    "        \n",
    "        output_patches = (self._get_image_patches(output_grad)\n",
    "                          .transpose(1, 0, 2, 3, 4)\n",
    "                          .reshape(batch_size * img_size, -1))\n",
    "        \n",
    "        param_reshaped = (self.param\n",
    "                          .reshape(self.param.shape[0], -1)\n",
    "                          .transpose(1, 0))\n",
    "\n",
    "        return (np.matmul(output_patches, param_reshaped)\n",
    "                .reshape(batch_size, img_height, img_height, self.param.shape[0])\n",
    "                .transpose(0, 3, 1, 2))\n",
    "        \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        BACKWARD PASS!\n",
    "        \n",
    "        1. step: Get image patches of size [batch_size, in_channels, img_height * img_width, kernel_size, kernel_size]\n",
    "        2. step: reshape image patches to [in_channels * param_height * param_width, batch_size * img_height * img_width]\n",
    "        3. step: reshape tthe output to [batch_size * img_height * img_width, out_channels]\n",
    "        4. step: batch matrix multiplication [in_channels * param_height * param_width, out_channels]\n",
    "        5. step: reshape the parameter gradient to [in_channels, out_channels, param_height, param_width]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self.input_.shape[0]\n",
    "\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "\n",
    "        in_channels = self.param.shape[0]\n",
    "        out_channels = self.param.shape[1]\n",
    "        \n",
    "        #step 1 and step 2 and step 3\n",
    "        in_patches_reshape = (self._get_image_patches(self.input_)\n",
    "                                        .reshape(batch_size * img_size, -1)\n",
    "                                        .transpose(1,0))\n",
    "        \n",
    "        # transpose to match general pattern for backward gradient\n",
    "        out_grad_reshape = output_grad.transpose(0,2,3,1).reshape(batch_size * img_size, -1)\n",
    "        \n",
    "        # step 4 and step 5\n",
    "        return (np.matmul(in_patches_reshape, out_grad_reshape)\n",
    "                        .reshape(in_channels, self.param_size, self.param_size, out_channels)\n",
    "                        .transpose(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_.reshape(self.input_.shape[0], -1)\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad.reshape(self.input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "#TODO: proper implementation - this is copy/paste\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def _output(self) -> float:\n",
    "        \n",
    "        if self.target.shape[1] == 0:\n",
    "            raise NotImplementedError()     \n",
    "\n",
    "        softmax_preds = softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        )\n",
    "        \n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "    \n",
    "    def _input_grad(self) -> ndarray:\n",
    "\n",
    "        return (self.softmax_preds - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is no beauty, but it works!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steffen/Documents/deep_learning_fundamentals/utils/operations.py:162: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-1.0 * self.input_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 9.986\n",
      "Validation loss after 2 epochs is 8.988\n",
      "Validation loss after 3 epochs is 10.053\n",
      "Validation loss after 4 epochs is 7.842\n",
      "Validation loss after 5 epochs is 9.555\n",
      "Validation loss after 6 epochs is 9.809\n",
      "Validation loss after 7 epochs is 10.407\n",
      "Validation loss after 8 epochs is 9.666\n",
      "Validation loss after 9 epochs is 8.497\n",
      "Validation loss after 10 epochs is 8.726\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "            layers=[Conv2D(out_channels=16, param_size=5, flatten=True, activation=Tanh()),\n",
    "                    Dense(neurons=10,activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy())\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr = 0.1))\n",
    "\n",
    "trainer.fit(X_train_conv, train_labels, X_test_conv, test_labels,\n",
    "            epochs = 10,\n",
    "            eval_every = 1,\n",
    "            batch_size=60);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
