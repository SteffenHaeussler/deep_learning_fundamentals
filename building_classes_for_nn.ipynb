{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    \n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        \"\"\"\n",
    "        Two ndarrays should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        \"\"\".format(tuple(array.shape), tuple(array_grad.shape))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class\n",
    "\n",
    "mental model for all operations that will encounter throughout deep learning. Sends inputs forward and gradients backward and tests, if shapes are matching.\n",
    "\n",
    "Operation class is needed for:\n",
    "   - activation function\n",
    "   \n",
    "ParamOperation class is needed for:\n",
    "   - weight multiplication\n",
    "   - bias addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation():\n",
    "    \"\"\"Base class for an artificial neural network\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        stores input in the self.input_ instance variable\n",
    "        calls \"\"\"\n",
    "        self.input_ = input_\n",
    "        \n",
    "        self.output = self._output()\n",
    "        \n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        calls the _input_grad function\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    \"\"\"\n",
    "    allows operation with parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Calls self._input and self._param_grad\n",
    "        \"\"\"\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks\n",
    "\n",
    "there are three kinds if blocks:\n",
    "\n",
    "- matrix multiplication of the input with the parameter matrix\n",
    "- addition of the bias term\n",
    "- activation function (here sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    \"\"\"Weight multiplication for a neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, W: ndarray):\n",
    "        \"\"\"Init Operation with self.param = W\"\"\"\n",
    "        super().__init__(W)\n",
    "        \n",
    "    def _output(self) -> ndarray:\n",
    "        \n",
    "        return np.dot(self.input_, self.param)\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        return np.dot(output_grad, np.transpose(self.param, (1,0)))\n",
    "        \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        return np.dot(np.transpose(self.input_,(1,0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    \"\"\"Add bias\"\"\"\n",
    "    \n",
    "    def __init__(self, B: ndarray):\n",
    "        \"\"\"Init Operation with self.param = B\"\"\"\n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "        \n",
    "    def _output(self) -> ndarray:\n",
    "        \n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "        \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self) -> ndarray:\n",
    "        return 1.0/(1.0 + np.exp(-1.0 * self.input_))\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        sigmoid_backward = self.output * (1 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    \"\"\"\n",
    "    Identity\" activation function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Blueprint\n",
    "\n",
    "- forward and backward methods simply involve sending the input forwards through a series of operations\n",
    "\n",
    "    - defining the correct series of operations in a setup_layer function and initializing and storing the parameters in these operations\n",
    "    \n",
    "    - storing the correct values in self.input_ and self.output on the forward method\n",
    "    \n",
    "    - performing the correct assertion checking in the backward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"layer in a neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, neurons: int):\n",
    "        \n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grad: List[ndarray] = []\n",
    "        self.operations: List[ndarray] = []\n",
    "            \n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "            \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations:\n",
    "            \n",
    "            input_ = operation.forward(input_)\n",
    "            \n",
    "        self.output = input_\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "            \n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "        \n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self) -> ndarray:\n",
    "        \n",
    "        self.param_grads = []\n",
    "        \n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "                \n",
    "    def _params(self) -> ndarray:\n",
    "        \n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"fully connected layer\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()) -> None:\n",
    "        \"\"\"requires an activation function upon initialization\"\"\"\n",
    "        \n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        defines options for a fully connected layer\n",
    "        \"\"\"\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "        self.params = []\n",
    "        \n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "        \n",
    "        #bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \"\"\"loss calculation of the network\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        \"\"\"\n",
    "        computes the actual loss value\n",
    "        \"\"\"\n",
    "        assert_same_shape(prediction, target)\n",
    "        \n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        loss_value = self._output()\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        computes gradient of the loss value with respect to the input of the loss function\n",
    "        \"\"\"\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.target)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        \"\"\"\n",
    "        every subclass of \"loss\" had to implement the output function!\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self) -> float:\n",
    "        \"\"\"\n",
    "        every subclass of \"loss\" had to implement the input_grad function!\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self) -> float:\n",
    "        \"\"\"\n",
    "        computes the observation squared error loss\n",
    "        \"\"\"\n",
    "        loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        calculates the loss gradient with respect to the input of the mse loss\n",
    "        \"\"\"\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"NeuralNetwork\" class\n",
    "\n",
    "Basically the class should take batches of observations and targets und learn the relationship between X and y.\n",
    "With the Layer and Operation classes, it needs following:\n",
    "\n",
    "   - take X and pass it forward through each layer until the result presents the prediction\n",
    "   - prediction should be compared to the true value to calculate the loss and loss gradient. (partial derivative of the loss with respect to each element in the last layer of the network)\n",
    "   - the loss gradient will be send backward though each layer, along the way computing the parameter gradients. (partial derivatives of the loss with respect to each parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: float=1):\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)\n",
    "    \n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        passes data forward through the series of layers\n",
    "        \"\"\"\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        passes loss gradient backward through the series of layers\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = loss_grad\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Passes data forward through the layers.\n",
    "        Computes the loss.\n",
    "        Passes data backward through the layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = self.forward(x_batch)\n",
    "\n",
    "        loss = self.loss.forward(prediction, y_batch)\n",
    "\n",
    "        self.backward(self.loss.backward())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Gets parameters for the network.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        \"\"\"\n",
    "        Gets the gradient of the loss with respect to the parameters for the network.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Base class for an optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochasitc gradient descent optimizer.\n",
    "    \"\"\"    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update each parameter ased on the learning rate.\n",
    "        \"\"\"\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trains a neural network\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer) -> None:\n",
    "        \"\"\"\n",
    "        Assign the neural network as an instance variable to the optimizer.\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "        \n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        \"\"\"\n",
    "        Generates training batches  \n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "            \"\"\"\n",
    "            features and target must have the same number of rows, instead\n",
    "            features has {0} and target has {1}\n",
    "            \"\"\".format(X.shape[0], y.shape[0])\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_valid: ndarray, y_valid: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True)-> None:\n",
    "        \"\"\"\n",
    "        Fits the neural network on the training data\n",
    "        Every \"eval_every\" epochs, it evaluated the neural network on the validation data.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "        for e in range(epochs):\n",
    "                \n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                valid_preds = self.net.forward(X_valid)\n",
    "                loss = self.net.loss.forward(valid_preds, y_valid)\n",
    "\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_np(a: ndarray, \n",
    "          type: str=\"col\") -> ndarray:\n",
    "    '''\n",
    "    Turns a 1D Tensor into 2D\n",
    "    '''\n",
    "\n",
    "    assert a.ndim == 1, \\\n",
    "    \"Input tensors must be 1 dimensional\"\n",
    "    \n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "\n",
    "    prediction = model.forward(X_test)\n",
    "    prediction = prediction.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mean_absolute_error(prediction, y_test)))\n",
    "    print(\"\\nRoot mean squared error {:.2f}\".format(mean_squared_error(prediction, y_test, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regresion = NeuralNetwork(layers=[Dense(neurons=1,\n",
    "                                               activation=Linear())],\n",
    "                                loss=MeanSquaredError(),\n",
    "                                seed=1\n",
    "                                )\n",
    "\n",
    "neural_network = NeuralNetwork(layers=[Dense(neurons=13,\n",
    "                                            activation=Sigmoid()),\n",
    "                                      Dense(neurons=1,\n",
    "                                            activation=Linear())],\n",
    "                                loss=MeanSquaredError(),\n",
    "                                seed=1\n",
    "                                )\n",
    "\n",
    "neural_network_2 = NeuralNetwork(layers=[Dense(neurons=13,\n",
    "                                            activation=Sigmoid()),\n",
    "                                         Dense(neurons=13,\n",
    "                                            activation=Sigmoid()),\n",
    "                                      Dense(neurons=1,\n",
    "                                            activation=Linear())],\n",
    "                                loss=MeanSquaredError(),\n",
    "                                seed=1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "# make target 2d array\n",
    "y_train, y_val, y_test = to_2d_np(y_train), to_2d_np(y_val), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 41.396\n",
      "Validation loss after 20 epochs is 21.537\n",
      "Validation loss after 30 epochs is 22.436\n",
      "Validation loss after 40 epochs is 23.285\n",
      "Validation loss after 50 epochs is 23.175\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(linear_regresion, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_val, y_val,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 3.44\n",
      "\n",
      "Root mean squared error 4.50\n"
     ]
    }
   ],
   "source": [
    "eval_model(linear_regresion, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 31.520\n",
      "Validation loss after 20 epochs is 26.878\n",
      "Validation loss after 30 epochs is 22.657\n",
      "Validation loss after 40 epochs is 20.736\n",
      "Validation loss after 50 epochs is 18.417\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(neural_network, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_val, y_val,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 2.92\n",
      "\n",
      "Root mean squared error 4.02\n"
     ]
    }
   ],
   "source": [
    "eval_model(neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 38.433\n",
      "Validation loss after 20 epochs is 28.609\n",
      "Validation loss after 30 epochs is 19.886\n",
      "Validation loss after 40 epochs is 18.124\n",
      "Validation loss after 50 epochs is 15.767\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(neural_network_2, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_val, y_val,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 2.53\n",
      "\n",
      "Root mean squared error 3.48\n"
     ]
    }
   ],
   "source": [
    "eval_model(neural_network_2, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
